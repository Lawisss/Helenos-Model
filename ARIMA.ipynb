{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle ARIMA sur les contrats futures S&P500 (E-minis - ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'élaborer notre stratégie de trading à partir du modèle prédictif (ARIMA/GARCH) nous allons d'abord réaliser une prédiction des E-minis avec le modèle ARIMA.\n",
    "\n",
    "Pour ce faire nous passerons par 7 étapes :\n",
    "\n",
    "1. Le sourcing des futures S&P500 (E-minis)\n",
    "2. La conception des visualisations des simulations\n",
    "3. La simulation du processus stochastique ARIMA\n",
    "4. Détermination du critère d'information d'Akaike (AIC)\n",
    "5. Le Data fitting avec avec test de Ljung-Box\n",
    "6. La vérification de la normalité avec le test de Jarque-Bera\n",
    "7. La prédiction et backtesting du modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer les librairies utilisées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "import statsmodels.stats as sms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le sourcing des futures S&P500 (E-minis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le sourcing des données porte sur les futures S&P500 (E-minis) du **18 septembre 2000 à aujourd'hui**. \n",
    "\n",
    "Pour faire fonctionner notre modèle nous utiliserons \n",
    "\n",
    "- **80 % de données d'entrainement (jusqu'au 18 septembre 2016 ~ 5000 données)** \n",
    "- **20 % de données de test (du 19 septembre 2016 à aujourd'hui ~ 1500 données)**.\n",
    "\n",
    "Nous sourçons nos données grâce à notre fournisseur **Yahoo Finance** qui nous permet d'exploiter environ **6500 données**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for stock: ES=F\n",
      "20% done...\n",
      "40% done...\n",
      "60% done...\n",
      "80% done...\n"
     ]
    }
   ],
   "source": [
    "from backtester.dataSource.yahoo_data_source import YahooStockDataSource\n",
    "startDateStr = '2000/09/18'\n",
    "endDateStr = '2016/09/18'\n",
    "cachedFolderName = 'yahooData/'\n",
    "dataSetId = 'Futures_ES_2000_2016'\n",
    "instrumentIds = ['ES=F']\n",
    "ds = YahooStockDataSource(cachedFolderName=cachedFolderName,\n",
    "                            dataSetId=dataSetId,\n",
    "                            instrumentIds=instrumentIds,\n",
    "                            startDateStr=startDateStr,\n",
    "                            endDateStr=endDateStr,\n",
    "                            event='history')\n",
    "data = ds.getBookDataByFeature()['adjClose']\n",
    "\n",
    "# log returns\n",
    "lrets = np.log(data/data.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pourquoi utiliser le logarithme des rendements ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par définir un retour : $r_i$ au moment $i$, où $pi$ est le prix au moment $i$ et $j \\equiv (i - 1)$ :\n",
    "\n",
    "$$r_i = \\frac {p_i - p_j}{p_j}$$\n",
    "   \n",
    "\n",
    "L'avantage de l'utilisation des ***rendements*** , par rapport aux prix, est la ***normalisation*** : mesurer toutes les variables dans une métrique comparable, permettant ainsi l'évaluation des relations analytiques entre deux ou plusieurs variables bien qu'elles proviennent de séries de prix de valeurs inégales. C'est une exigence pour de nombreuses techniques d'analyse statistique multidimensionnelle et d'apprentissage automatique. Par exemple, l'interprétation d'une matrice de covariance d'équité est rendue sensée lorsque les variables sont toutes deux mesurées en pourcentage.\n",
    "\n",
    "Plusieurs avantages de l'utilisation des ***retours de log***, à la fois théoriques et algorithmiques.\n",
    "\n",
    "Premièrement, la ***normalité logarithmique*** : si nous supposons que les prix sont distribués log normalement (ce qui, en pratique, peut ou non être vrai pour une série de prix donnée), alors $log(1 + r_i)$ est commodément distribué normalement, car:\n",
    "\n",
    "$$1 + r_i = \\frac{p_i}{p_j} = \\exp^{\\log(\\frac{p_i}{p_j})}$$  \n",
    "\n",
    "C'est pratique étant donné que la plupart des statistiques classiques présument la normalité.\n",
    "\n",
    "Deuxièmement, ***l'égalité approximative du log brut*** : lorsque les rendements sont très faibles (courant pour les transactions avec des durées de détention courtes), l'approximation suivante garantit qu'ils sont proches en valeur des rendements bruts:\n",
    "\n",
    "$$\\log(1 + r) \\approx r $$avec$$ r \\ll 1$$ \n",
    "\n",
    "Troisièmement, ***l'additivité au temps*** : considérez une séquence ordonnée de $n$ transactions. Une statistique fréquemment calculée à partir de cette séquence est le ***rendement composé***, qui est le rendement courant de cette séquence de transactions au fil du temps :\n",
    "\n",
    "$$\\displaystyle(1 + r_1)(1 + r_2)\\cdots(1 + r_n) = \\prod_i(1 + r_i)$$\n",
    "\n",
    "Cette formule est assez désagréable, car la théorie des probabilités nous le rappelle que le produit de variables normalement distribuées n'est pas normal. Au lieu de cela, la somme des variables normalement distribuées est normale (technicité importante: uniquement lorsque toutes les variables ne sont pas corrélées ), ce qui est utile lorsque l'on rappelle l'identité logarithmique suivante :\n",
    "\n",
    "$$\\log (1 + r_i) = log (\\frac {p_i}{p_j}) = \\log (p_i) - log (p_j)$$ \n",
    "\n",
    "Ainsi, les rendements composés sont normalement distribués. Enfin, cette identité nous conduit à un bénéfice algorithmique agréable; une formule simple pour calculer les rendements composés:\n",
    "\n",
    "$$\\displaystyle \\sum_i \\log (1 + r_i) = \\log (1 + r_1) + \\log (1 + r_2) + \\cdots + \\log (1 + r_n) = \\log (p_n) - \\log (p_0)$$\n",
    "\n",
    "Ainsi, le rendement composé sur $n$ périodes est simplement la différence de log entre les périodes initiale et finale. En termes de complexité algorithmique , cette simplification réduit O (n) multiplications à O (1) additions. C'est une énorme victoire pour n modéré à grand. En outre, cette somme est utile pour les cas où les rendements divergent de la normale, car le ***théorème de la limite centrale*** nous rappelle que la moyenne de l'échantillon de cette somme convergera vers la normalité (en supposant des premier et deuxième moments finis).\n",
    "\n",
    "Quatrièmement, la ***facilité mathématique*** : du calcul, on nous rappelle (en ignorant la constante d'intégration) :\n",
    "\n",
    "$$e ^ x = \\int e ^ x dx = \\frac {d}{dx} e ^ x = e ^ x$$ \n",
    "\n",
    "Cette identité est extrêmement utile, car une grande partie des mathématiques financières est construite sur des ***processus stochastiques en temps continu*** qui reposent fortement sur l'intégration et la différenciation.\n",
    "\n",
    "Cinquièmement, la stabilité numérique : l'addition de petits nombres est numériquement sûre, alors que la multiplication de petits nombres ne l'est pas car elle est sujette à un ***dépassement arithmétique***. Pour de nombreux problèmes intéressants, il s'agit d'un problème potentiel sérieux. Pour résoudre ce problème, soit l'algorithme doit être modifié pour être numériquement robuste, soit il peut être transformé en une somme numériquement sûre via des journaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La conception des visualisations des simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La simulation du processus stochastique ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détermination du critère d'information d'Akaike (AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le Data fitting avec avec test de Ljung-Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La vérification de la normalité avec le test de Jarque-Bera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La prédiction et backtesting du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
