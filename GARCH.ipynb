{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle GARCH sur les contrats futures S&P500 (E-minis - ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'élaborer notre stratégie de trading à partir du modèle prédictif (ARIMA/GARCH) nous allons d'abord réaliser une prédiction des E-minis avec le modèle GARCH.\n",
    "\n",
    "Pour ce faire nous passerons par 4 étapes :\n",
    "\n",
    "1. Le sourcing des futures S&P500 (E-minis)\n",
    "2. La conception des visualisations des simulations\n",
    "3. La simulation du processus stochastique ARIMA\n",
    "    * Détermination du critère d'information d'Akaike (AIC)\n",
    "    * Tableau des résultats\n",
    "    * Le Data fitting avec avec test de Ljung-Box\n",
    "    * La vérification de la normalité avec le test de Jarque-Bera\n",
    "4. La prédiction et backtesting du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer les librairies utilisées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "import statsmodels.stats as sms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le sourcing des futures S&P500 (E-minis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le sourcing des données porte sur les futures S&P500 (E-minis) du **18 septembre 2000 à aujourd'hui**. \n",
    "\n",
    "Pour faire fonctionner notre modèle nous utiliserons \n",
    "\n",
    "- **80 % de données d'entrainement (jusqu'au 18 septembre 2016 ~ 5000 données)** \n",
    "- **20 % de données de test (du 19 septembre 2016 à aujourd'hui ~ 1500 données)**.\n",
    "\n",
    "Nous sourçons nos données grâce à notre fournisseur **Yahoo Finance** qui nous permet d'exploiter environ 6500 données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for stock: ES=F\n",
      "20% done...\n",
      "40% done...\n",
      "60% done...\n",
      "80% done...\n"
     ]
    }
   ],
   "source": [
    "from backtester.dataSource.yahoo_data_source import YahooStockDataSource\n",
    "startDateStr = '2000/09/18'\n",
    "endDateStr = '2016/09/18'\n",
    "cachedFolderName = 'yahooData/'\n",
    "dataSetId = 'Futures_ES_2000_2016'\n",
    "instrumentIds = ['ES=F']\n",
    "ds = YahooStockDataSource(cachedFolderName=cachedFolderName,\n",
    "                            dataSetId=dataSetId,\n",
    "                            instrumentIds=instrumentIds,\n",
    "                            startDateStr=startDateStr,\n",
    "                            endDateStr=endDateStr,\n",
    "                            event='history')\n",
    "data = ds.getBookDataByFeature()['adjClose']\n",
    "\n",
    "# log returns\n",
    "lrets = np.log(data/data.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pourquoi utiliser le logarithme des rendements ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par définir un retour : $r_i$ au moment $i$, où $pi$ est le prix au moment $i$ et $j \\equiv (i - 1)$ :\n",
    "\n",
    "$$r_i = \\frac {p_i - p_j}{p_j}$$\n",
    "   \n",
    "\n",
    "L'avantage de l'utilisation des ***rendements*** , par rapport aux prix, est la ***normalisation*** : mesurer toutes les variables dans une métrique comparable, permettant ainsi l'évaluation des relations analytiques entre deux ou plusieurs variables bien qu'elles proviennent de séries de prix de valeurs inégales. C'est une exigence pour de nombreuses techniques d'analyse statistique multidimensionnelle et d'apprentissage automatique. Par exemple, l'interprétation d'une matrice de covariance d'équité est rendue sensée lorsque les variables sont toutes deux mesurées en pourcentage.\n",
    "\n",
    "Plusieurs avantages de l'utilisation des ***retours de log***, à la fois théoriques et algorithmiques.\n",
    "\n",
    "Premièrement, la ***normalité logarithmique*** : si nous supposons que les prix sont distribués log normalement (ce qui, en pratique, peut ou non être vrai pour une série de prix donnée), alors $log(1 + r_i)$ est commodément distribué normalement, car:\n",
    "\n",
    "$$1 + r_i = \\frac{p_i}{p_j} = \\exp^{\\log(\\frac{p_i}{p_j})}$$  \n",
    "\n",
    "C'est pratique étant donné que la plupart des statistiques classiques présument la normalité.\n",
    "\n",
    "Deuxièmement, ***l'égalité approximative du log brut*** : lorsque les rendements sont très faibles (courant pour les transactions avec des durées de détention courtes), l'approximation suivante garantit qu'ils sont proches en valeur des rendements bruts:\n",
    "\n",
    "$$\\log(1 + r) \\approx r $$avec$$ r \\ll 1$$ \n",
    "\n",
    "Troisièmement, ***l'additivité au temps*** : considérez une séquence ordonnée de $n$ transactions. Une statistique fréquemment calculée à partir de cette séquence est le ***rendement composé***, qui est le rendement courant de cette séquence de transactions au fil du temps :\n",
    "\n",
    "$$\\displaystyle(1 + r_1)(1 + r_2)\\cdots(1 + r_n) = \\prod_i(1 + r_i)$$\n",
    "\n",
    "Cette formule est assez désagréable, car la théorie des probabilités nous le rappelle que le produit de variables normalement distribuées n'est pas normal. Au lieu de cela, la somme des variables normalement distribuées est normale (technicité importante: uniquement lorsque toutes les variables ne sont pas corrélées ), ce qui est utile lorsque l'on rappelle l'identité logarithmique suivante :\n",
    "\n",
    "$$\\log (1 + r_i) = log (\\frac {p_i}{p_j}) = \\log (p_i) - log (p_j)$$ \n",
    "\n",
    "Ainsi, les rendements composés sont normalement distribués. Enfin, cette identité nous conduit à un bénéfice algorithmique agréable; une formule simple pour calculer les rendements composés:\n",
    "\n",
    "$$\\displaystyle \\sum_i \\log (1 + r_i) = \\log (1 + r_1) + \\log (1 + r_2) + \\cdots + \\log (1 + r_n) = \\log (p_n) - \\log (p_0)$$\n",
    "\n",
    "Ainsi, le rendement composé sur $n$ périodes est simplement la différence de log entre les périodes initiale et finale. En termes de complexité algorithmique , cette simplification réduit O (n) multiplications à O (1) additions. C'est une énorme victoire pour n modéré à grand. En outre, cette somme est utile pour les cas où les rendements divergent de la normale, car le ***théorème de la limite centrale*** nous rappelle que la moyenne de l'échantillon de cette somme convergera vers la normalité (en supposant des premier et deuxième moments finis).\n",
    "\n",
    "Quatrièmement, la ***facilité mathématique*** : du calcul, on nous rappelle (en ignorant la constante d'intégration) :\n",
    "\n",
    "$$e ^ x = \\int e ^ x dx = \\frac {d}{dx} e ^ x = e ^ x$$ \n",
    "\n",
    "Cette identité est extrêmement utile, car une grande partie des mathématiques financières est construite sur des ***processus stochastiques en temps continu*** qui reposent fortement sur l'intégration et la différenciation.\n",
    "\n",
    "Cinquièmement, la stabilité numérique : l'addition de petits nombres est numériquement sûre, alors que la multiplication de petits nombres ne l'est pas car elle est sujette à un ***dépassement arithmétique***. Pour de nombreux problèmes intéressants, il s'agit d'un problème potentiel sérieux. Pour résoudre ce problème, soit l'algorithme doit être modifié pour être numériquement robuste, soit il peut être transformé en une somme numériquement sûre via des journaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La conception des visualisations des simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ci-dessous nous permet d'afficher 5 graphiques : \n",
    "\n",
    "- **La série temporelle**\n",
    "- **L'ACF**\n",
    "- **Le PACF**\n",
    "- **Le diagramme Quantile-Quantile**\n",
    "- **La courbe de probabilité normale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **PACF** est une **corrélation partielle** entre les résidus, qui tient compte des délais plus courts. L'axe des $x$ indique les valeurs du décalage, $k$ et la région ombrée en bleu indique les **limites de signification**. Seules les valeurs qui se situent en dehors de la région bleue sont **significatives**.\n",
    "\n",
    "Le **diagramme Quantile-Quantile** compare deux distributions de probabilité en traçant leurs quantiles l'un par rapport à l'autre. Si les deux distributions sont similaires (ou linéairement liées), les points du tracé Q-Q se situeront approximativement sur une ligne. Nous comparons la distribution de nos résidus avec une distribution normale.\n",
    "\n",
    "La **courbe de probabilité** normale évalue également si un ensemble de données est ou non distribué approximativement normalement. Les données sont tracées par rapport à une distribution normale théorique de telle manière que les points doivent former une ligne droite approximative. Les écarts par rapport à cette ligne droite indiquent des écarts par rapport à la normalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 8), style='seaborn-darkgrid'):\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    \n",
    "    #Configuration des graphiques\n",
    "        \n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (3, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        qq_ax = plt.subplot2grid(layout, (2, 0))\n",
    "        pp_ax = plt.subplot2grid(layout, (2, 1))\n",
    "        \n",
    "    #Réalisation des graphiques\n",
    "            \n",
    "        y.plot(ax=ts_ax)\n",
    "        ts_ax.set_title('Time Series Analysis Plots')\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)\n",
    "        sm.qqplot(y, line='s', ax=qq_ax)\n",
    "        qq_ax.set_title('QQ Plot')        \n",
    "        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La simulation du processus stochastique GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout comme ARCH (p) est AR (p) appliqué à la variance d'une série chronologique, **GARCH (p, q)** est un modèle ARMA (p, q) appliqué à la **variance** d'une série chronologique. \n",
    "L'AR (p) modélise la variance des résidus (erreurs au carré) ou simplement notre série chronologique au carré. La partie MA (q) modélise la variance du processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Détermination du critère d'information d'Akaike (AIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le critère **AIC** est une **mesure de la qualité d'un modèle statistique**. Il s’applique aux modèles estimés par un **maximum de vraisemblance** : les analyses de variance, les régressions linéaires multiples, les régressions logistiques et de Poisson peuvent rentrer dans ce cadre. Il utilise ce maximum tout en pénalisant les modèles comportant trop de variables, qui **surapprennent** les données et généralisent mal.\n",
    "\n",
    "Le critère AIC est défini par : $$AIC = −2 log (L) + 2k$$ \n",
    "\n",
    "où $L$ est la vraisemblance maximisée et $k$ le nombre de paramètres dans le modèle. \n",
    "\n",
    "Avec ce critère, la déviance du modèle $−2 log (L)$ est pénalisée par **2 fois** le nombre de paramètres.\n",
    "\n",
    "L’AIC représente donc un compromis entre le **biais**, diminuant avec le nombre de paramètres, et la **parcimonie**, volonté de décrire les données avec le plus petit nombre de paramètres possibles. \n",
    "\n",
    "- La rigueur voudrait que tous les modèles comparés dérivent tous d’un même « complet » inclus dans la liste des modèles comparés.\n",
    "- Il est nécessaire de vérifier que les conditions d’utilisation du modèle complet et de celui sélectionné sont remplies.\n",
    "- Le **meilleur modèle** est celui possédant l’AIC le plus **faible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le Data fitting avec avec test de Ljung-Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test de **Ljung-Box** est un test statistique qui teste **l'auto-corrélation d'ordre supérieur à 1** (dépendance). Il peut être défini comme :\n",
    "\n",
    "$H_0$ (hypothèse nulle) : les données sont distribuées **indépendamment** (les corrélations dans la population à partir desquelles l'échantillon est prélevé sont égales à 0, de sorte que toute corrélation observée dans les données résulte du caractère aléatoire du processus d'échantillonnage).\n",
    "\n",
    "$H_a$ (hypothèse alternative) : les données ne sont pas distribuées indépendamment; ils présentent une **corrélation** en série.\n",
    "\n",
    "La statistique du test est:\n",
    "\n",
    "$$Q = n\\left(n+2\\right)\\sum_{k=1}^h\\frac{\\hat{\\rho}^2_k}{n-k}$$\n",
    "\n",
    "où $n$ est la taille de l'échantillon, $\\hat{\\rho}_k$ est l'autocorrélation de l'échantillon au décalage $k$ , et $h$ est le nombre de décalages testés. En dessous de $H_0$ la statistique $Q$ suit asymptotiquement un $\\chi _{{(h)}}^{2}$. \n",
    "\n",
    "Pour le niveau de signification $\\alpha$, la région critique pour le rejet de l'hypothèse du caractère aléatoire est:\n",
    "$Q > \\chi_{1-\\alpha,h}^2$ où $\\chi_{1-\\alpha,h}^2$ est le quantile $1 - \\alpha$ de la distribution $\\chi_{2}$ avec $h$ degrés de liberté.\n",
    "\n",
    "\n",
    "Dans notre cas, on retrouve en paramètre notre meilleur modèle : **ARIMA (3,0,3)**. Le test est appliqué aux résidus d'un modèle ARIMA ajusté, et non à la série d'origine, et dans de telles applications, l'hypothèse $H_0$ actuellement testée est que **les résidus du modèle ARIMA n'ont pas d'autocorrélation** (contrairement à l'hypothèse $H_0$ du test Ljung-Box général). \n",
    "\n",
    "Lors du test des résidus d'un modèle ARIMA estimé, les degrés de liberté doivent être ajustés pour refléter l'estimation des paramètres. Par exemple, pour un modèle ARIMA (p, 0, q), les degrés de liberté doivent être définis sur $h$ - $p$ - $q$.\n",
    "\n",
    "De ce test nous sommes censé obtenir la **statistique du test** et la **p-value**.\n",
    "\n",
    "Si la **p-value** < $0,05$ : Nous **rejetons l'hypothèse nulle** en supposant qu'il y 5% de chances de faire une erreur. Nous supposons que nos valeurs montrent une **dépendance** les unes envers les autres.\n",
    "\n",
    "Si la **p-value** > $0,05$ : Nous n'avons pas suffisamment de preuves statistiques pour rejeter l'hypothèse nulle. Nous ne pouvons donc pas supposer que nos valeurs sont dépendantes. Cela peut signifier que nos valeurs sont **indépendantes** ou qu'elles **dépendent de toute façon**. En effet, nous ne pouvons pas prouver de possibilités spécifiques, c'est à dire que nous ne pouvons pas affirmer la dépendance des valeurs, ni affirmer l'indépendance des valeurs (non confirmation de l'hypothèse nulle).\n",
    "\n",
    "\n",
    "Comme expliqué au début, nous ne pouvons pas prouver l'indépendance des valeurs des séries temporelles à l'aide du test Ljung-Box mais **seulement** la dépendance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_mdl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0c9145e3a490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Décalage 20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiagnostic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macorr_ljungbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_mdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxpierce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_mdl' is not defined"
     ]
    }
   ],
   "source": [
    "# Data fitting avec Ljung-Box\n",
    "# Degrés de liberté = 14\n",
    "# Décalage 20\n",
    "\n",
    "sms.diagnostic.acorr_ljungbox(best_mdl.resid, lags=[20], boxpierce=False, return_df=True, model_df=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi nous retrouvons la statistique du test de Ljung-Box égale à $25$ et la p-value égale à $0.02$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La vérification de la normalité avec le test de Jarque-Bera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test de Jarque-Bera est un test d'hypothèse qui cherche à déterminer si des données suivent une **loi normale**.\n",
    "\n",
    "Comme pour chaque test d'hypothèse, il faut poser une hypothèse nulle à valider :\n",
    "\n",
    "$H_{0}$ : les données suivent une loi normale $(S=0{\\mbox{ et }}K=3)$.\n",
    "\n",
    "$H_{1}$ : les données ne suivent pas une loi normale $(S\\neq 0{\\mbox{ ou }}K\\neq 3)$.\n",
    "\n",
    "\n",
    "La variable de Jarque-Bera s'écrit :\n",
    "\n",
    "$$\\mathit{JB}=\\frac{n-k}{6}\\left(S^{2}+{\\frac{(K-3)^{2}}{4}}\\right)$$\n",
    "\n",
    "avec :\n",
    "\n",
    "- $n$, le nombre d'observations\n",
    "- $k$, le nombre de variables explicatives si les données proviennent des résidus d'une régression linéaire. Sinon, k reste nul.\n",
    "- $S$, le coefficient d'asymétrie de l'échantillon testé.\n",
    "- $K$, la kurtosis de l'échantillon testé (**coefficient d’aplatissement**).\n",
    "\n",
    "Mathématiquement, $S$ et $K$ sont définis par:\n",
    "\n",
    "$\\begin{aligned}&S={\\frac {{\\hat {\\mu }}_{3}}{{\\hat {\\sigma }}^{3}}}={\\frac {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{3}}{\\left({\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}\\right)^{3/2}}}\\\\&K={\\frac {{\\hat {\\mu }}_{4}}{{\\hat {\\sigma }}^{4}}}={\\frac {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{4}}{\\left({\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}\\right)^{2}}},\\end{aligned}$\n",
    "\n",
    "\n",
    "\n",
    "avec $\\hat{\\mu_{3}}$ et $\\hat{\\mu_{4}}$ les estimateurs du troisième et quatrième moments, $\\bar{x}$ est la moyenne de l'échantillon et $\\hat{\\sigma}^2$ est la variance de l'échantillon.\n",
    "\n",
    "La statistique JB suit asymptotiquement une loi du χ² à deux degrés de liberté. Ce test est fréquemment utilisé pour déterminer si les **résidus** d'une régression linéaire suivent une **distribution normale**. \n",
    "\n",
    "Une loi normale a un **coefficient d'asymétrie** de **0** et une **kurtosis** de **3**. On saisit alors que si les données suivent une loi normale, le test s'approche alors de 0 et on accepte (ne rejette pas) $H_{0}$ au seuil $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test de normalité\n",
    "\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "score, pvalue, _, _ = jarque_bera(mdl.resid)\n",
    "\n",
    "if pvalue < 0.10:\n",
    "    print('Le résidu ne semble pas normalement distribué.')\n",
    "else:\n",
    "    print('Le résidu semble être distribué normalement.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La prédiction et backtesting du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
